{"data": "# fastapi-queue\n![fury](https://img.shields.io/pypi/v/fastapi-queue.svg)\n![licence](https://img.shields.io/github/license/GoodManWEN/fastapi-queue)\n![pyversions](https://img.shields.io/pypi/pyversions/fastapi-queue.svg)\n![Publish](https://github.com/GoodManWEN/fastapi-queue/workflows/Publish/badge.svg)\n![Build](https://github.com/GoodManWEN/fastapi-queue/workflows/Build/badge.svg)\n![Docs](https://readthedocs.org/projects/fastapi-queue/badge/?version=latest)\n\nA python implementation of a task queue based on `Redis` that can serve as a peak shaver and protect your app.\n\n[\u4e2d\u6587\u6587\u6863](https://github.com/GoodManWEN/fastapi-queue/tree/main/misc/readme_ch.md)\n\n## What is fastapi-queue?\nFastapi-queue provides a high-performance redis-based task queue that allows requests sent by clients to the `FastAPI` server to be cached in the queue for delayed execution. This means that you don't have to worry about overwhelming your back-end data service, nor do you have to worry about requests being immediately rejected due to exceeding the load limit, when there is an influx of requests to your app in a very short period of time.\n\n## Why fastapi-queue?\nThis module is for people who want to use task queues but don't want to start too many dependencies to prevent increased maintenance costs. For example if you want to enjoy the benefits of queues but want to maintain a lightweight application and don't want to install `Celery` & `RabbitMQ`, then fastapi-queue is your choice, you just need to rely on python runtime and `Redis` environment.\n\n## Feature\n- Separate gateway and service nodes.\n- Superior horizontal scalability.\n- Fully asynchronous framework, ultra fast.\n\n## Requirements\n- fastapi\n- aioredis >= 2.0.0\n- ThreadPoolExecutorPlus >= 0.2.2\n- msgpack >= 1.0.0\n\n## Install\n\n    pip install fastapi-queue\n\n## Documentation\n[https://fastapi-queue.readthedocs.io](https://fastapi-queue.readthedocs.io) \\(on going\\)\n\n## Response sequence description\n\n![](https://raw.githubusercontent.com/goodmanwen/fastapi-queue/main/misc/Schematic.png)\n\n## Examples\n\nGateway\n```python\n'''\nA gateway application made with FastAPI, which only handles whether or not to allow the \nrequest, but no need to handle the exact request logic.\n'''\nfrom typing import Optional, Any\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi_queue import DistributedTaskApplyManager\nimport aioredis\n\n\napp = FastAPI()\nredis = aioredis.Redis.from_url(\"redis://localhost\")\n\n\ndef get_response(success_status: bool, result: Any) -> JSONResponse | dict:\n    if success_status:\n        return {\"status\": 200, \"data\": result}\n    if result == -1:\n        return JSONResponse(status_code=503, content=\"Service Temporarily Unavailable\")\n    else:\n        return JSONResponse(status_code=500, content=\"Internal Server Error\")\n\n\n@app.get('/')\nasync def root(request: Request):\n    success_status: bool = False\n    async with DistributedTaskApplyManager(\n        redis = redis, \n        request_path = request.url.path,\n    ) as dtmanager:\n        if not dtmanager.success(): \n            # Exceed the maximum capacity of the back-end queue, return 503 directly.\n            return JSONResponse(status_code=503, content=\"Service Temporarily Unavailable\")\n        success_status, result = await dtmanager.rclt(form_data = {}, task_level = 0)\n    return get_response(success_status, result)\n\n\n@app.get('/sync-test')\nasync def sync_test(request: Request, x: int):\n    success_status: bool = False\n    async with DistributedTaskApplyManager(\n        redis = redis, \n        request_path = request.url.path,\n    ) as dtmanager:\n        if not dtmanager.success():\n            return JSONResponse(status_code=503, content=\"Service Temporarily Unavailable\")\n        success_status, result = await dtmanager.rclt(form_data = {'x': x}, task_level = 0)\n    return get_response(success_status, result)\n\n@app.get('/async-test')\nasync def async_test(request: Request, n: int):\n    n = min(n, 80)\n    success_status: bool = False\n    async with DistributedTaskApplyManager(\n        redis = redis, \n        request_path = request.url.path,\n    ) as dtmanager:\n        if not dtmanager.success():\n            return JSONResponse(status_code=503, content=\"Service Temporarily Unavailable\")\n        success_status, result = await dtmanager.rclt(form_data = {'n': n}, task_level = 0)\n    return get_response(success_status, result)\n```\n\nService nodes\n```python\n'''\nThe following code will create a pool of workers of 4 processes with 4 threads under \neach process. They rely on redis for synchronization, so you can run other instances \nas you like without worrying about creating conflicts.\n'''\nfrom fastapi_queue import QueueWorker\nfrom loguru import logger\nimport asyncio  \nimport aioredis\nimport signal \nimport sys \nimport os \n\nqueueworker = None\n\nasync def async_root(*args):\n    return \"Hello world.\"\n\ndef sync_prime_number(redis, mysql, x):\n    # Example synchronous function to determine if the input x is a prime number.\n    # redis and mysql clients are entered by default, starting from the third parameter \n    # is your custom parameters, only keyword parameters are supported. \n    import math, time\n    if x == 1:\n        return True\n    for numerator in range(2, int(math.sqrt(x))):\n        if x % numerator == 0:\n            return False\n    time.sleep(0.2) # Simulation of calculation time\n    return True\n\nasync def async_fibonacci(redis, mysql, n):\n    # Example asynchronous function to calculate the nth position of the Fibonacci series.\n    # redis and mysql clients are entered by default, starting from the third parameter \n    # is your custom parameters, only keyword parameters are supported.\n\n    # Be sure to note that all data to upload and download must be serializable by msgpack.\n    # This means that if you transfer some custom object, or in this case a very large integer, \n    # the request will be responsed with an internal server error (http 500).\n    a, b = 0, 1\n    for _ in range(n):\n        a, b = b, a + b \n    await asyncio.sleep(0.2) # Simulation of calculation time\n    return a \n\nroute_table = {\n    '/': async_root,\n    '/sync-test': sync_prime_number,\n    '/async-test': async_fibonacci,\n}\n\nroute_table_maximum_concurrency = {\n    '/': 9999,\n    '/sync-test': 320,\n    '/async-test': 1000,\n}\n\nasync def main(pid, logger):\n    global queueworker\n\n    first_time_run = True\n    while True:\n        run_startup, first_time_run = (True if pid != 0 else False) and first_time_run, False\n        redis = aioredis.Redis.from_url(\"redis://localhost\")\n        try:\n            worker = QueueWorker(\n                redis, \n                threads=4, \n                route_table_maximum_concurrency = route_table_maximum_concurrency, \n                allowed_type_limit=None, \n                run_startup=run_startup,\n                logger=logger,\n            )\n            queueworker = worker\n            [worker.method_register(name, func) for name, func in route_table.items()]\n            await worker.run_serve()\n            if worker.closeing():\n                break\n        except Exception as e:\n            debug = True\n            if debug:\n                raise e\n    await redis.close()\n    logger.info(f\"Pid: {worker.pid}, shutdown\")\n\n\ndef sigint_capture(sig, frame):\n    if queueworker: queueworker.graceful_shutdown(sig, frame)\n    else: sys.exit(1)\n\n\nif __name__ == '__main__':\n    logger.remove()\n    logger.add(sys.stderr, level=\"DEBUG\", enqueue=True)\n    signal.signal(signal.SIGINT, sigint_capture) # In order for the program to capture the `ctrl+c` close signal\n    for _ in range(3):\n        pid = os.fork()\n        if pid == 0: break\n    asyncio.run(main(pid, logger))\n```\n\n## Performance\n\nDue to the fully asynchronous support, complex interprocedural calls exhibit a very low processing latency.\n\n(Latency vs. number of request threads on going)\n\n(Maximum capability requests per second vs. number of service nodes on going)\n\n## Tips\n\n- The code has been carefully debugged and functions reliably, but I haven't spent much time making it a generic module, which means that if you encounter bugs you'll need to modify the code yourself, and they're usually caused by oversights of detail somewhere.\n- The service has undergone rigorous stress tests and can work for hours under concurrent requests from hundreds of clients, but for reliability of protection, you need to carefully set the upper limit of your load. Where `RateLimiter` can provide you with a low consumption **roughly** pre-intercepted function.\n\nFor example,\n```python\nfrom fastapi_queue import RateLimiter\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\n...\n\n@app.on_event(\"startup\")\nasync def startup():\n    RateLimiter().porter_run_serve()\n\n...\n\n@app.get(\"/\")\n@RateLimiter(bucket = 5000, limits_s = 1000)\nasync def root(request: Request):  \n    '''\n    The two parameters of RateLimiter mean that this particular FastAPI instance \n    holds a total of 5000 tokens and takes one token each time a request is received. \n    If there is a large influx of requests come in a short period of time, when \n    the number of remaining tokens in the bucket decreases to 0, the server will \n    simply reject all requests without forwarding them to the queue-worker. With \n    current parameters, this bucket now keeps a maximum of 5000 tokens and restores \n    1000 tokens per second.\n    '''\n    async with DistributedTaskApplyManager(\n        ...\n    )\n```\n"}